%% TODO:
% BibTeX MathWorld citation (Jishnu).

\documentclass[12pt,reqno,twoside,titlepage]{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[square,numbers,sort&compress]{natbib}

\usepackage{amsmath,amsfonts,graphicx,amssymb,amsthm,tikz,csquotes}

\usetikzlibrary{calc,arrows}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\theoremstyle{definition}
\newtheorem*{definition}{Definition}

\theoremstyle{theorem}
\newtheorem*{theorem}{Theorem}

\begin{document}

\title{Matrix Decompositions}
\author{George Keeble
  \and
  Jishnu Kaiwar
  \and
  Bing En Gan
  \and
  Jevvy Huang
}
\date{January 2019}

\maketitle

\begin{abstract}
  abstract here
\end{abstract}

\section{Introduction}
\label{sec:int}
the quick brown fox jumped over the lazy dog.

\section{Gaussian Elimination}
\label{sec:gaus}

Gaussian elimination (or row reduction) is used for solving systems of equations in linear algebra, it also plays a part in evaluating the rank, determinant and consequently the inverse of a matrix.
This algorithm is attributed to the mathematician Carl Gauss (1777-1855) however there is some speculation that some Chinese mathematicians were using this string of thought as early as 179 A.D. which I shall touch on breifly when discussing ‘Fangcheng’.
In order to compute the algorithm one must have a grounded understanding of elementary matrices and possible elementary row operations.
There are three elementary row operations;

\begin{enumerate}
\item Swapping two rows
\item Multiplying a row by $\lambda \in \mathbb{R}$ such that $\lambda \ne 0$
\item Adding a multiple of one row to another row
\end{enumerate}

We now need to define an upper triangular matrix, specifically what it means for it to be in row echelon form or reduced row echelon form.

\section{LU Decomposition}
\label{sec:LUD}
\subsection{Basic Concept}
\label{subsec:LUD:BAC}
The LU decomposition is the process of factorizing a given square matrix, $A$ into a unit lower triangular matrix, $L$ and an upper triangular matrix, $U$.
From \ref{sec:gaus}, the main point of Gaussian elimination is to convert a matrix into a triangular matrix.
The LU decomposition is an extension of the Gaussian elimination algorithm in order to take advantage of the triangular matrix system.
\newline
Suppose that we have a square matrix $A$, we can write:
$$A = LU,$$ 
where
\begin{equation*}
  L=\begin{pmatrix}
    1 & 0 & 0 \\
    l_{21} & 1 & 0 \\
    l_{31} & l_{32} & 1
  \end{pmatrix}
  ,
  \quad
  U=\begin{pmatrix}
    u_{11} & u_{12} & u_{13}\\
    0 & u_{22} & u_{23}\\
    0 & 0 & u_{33}
  \end{pmatrix}
\end{equation*}
\begin{center}
  \begin{equation*}
    l_{21}, l_{31}, l_{32}, u_{11}, u_{12}, u_{13}, u_{22}, u_{23}, u_{33} \in \mathbb{R};
  \end{equation*}
  \begin{equation*}
    u_{11}, u_{22}, u_{33} \neq 0
  \end{equation*}
\end{center}
The motivation of performing a LU decomposition is that triangular matrices are easier to deal with.
For example, solving linear system of equations, finding the inverse of a matrix, computing the determinant of a matrix, will be significantly easier with LU decomposition.
\paragraph{Remark}
We have to note that a matrix with determinant $0$ will not have a LU decomposition.
This is because we cannot have an element with value $0$ in the diagonal of both the triangular matrices.
\subsection{Applying Gaussian Elimination}
\label{subsec:LUD:AGE}
For an example, let
\begin{equation*}
  A=
  \begin{pmatrix}
    1 & 2 & 4\\
    3 & 7 & 13\\
    2 & 9 & 18
  \end{pmatrix}
\end{equation*}
and we want to find the LU decomposition of the matrix $A$.
First, we apply Gaussian elimination without swapping rows.
By subtracting 3 multiplied row 1 from row 2 and 2 multiplied row 1 from row 3,
\begin{equation*}
  \begin{pmatrix}
    1 & 2 & 4\\
    3 & 7 & 13\\
    2 & 9 & 18
  \end{pmatrix}
  \sim
  \begin{pmatrix}
    1 & 2 & 4\\
    0 & 1 & 1\\
    0 & 5 & 10
  \end{pmatrix}
\end{equation*}
Then, we subtract 5 multiplied row 2 from row 3,
\begin{equation*}
  \begin{pmatrix}
    1 & 2 & 4\\
    0 & 1 & 1\\
    0 & 5 & 10
  \end{pmatrix}
  \sim
  \begin{pmatrix}
    1 & 2 & 4\\
    0 & 1 & 1\\
    0 & 0 & 5
  \end{pmatrix}
\end{equation*}
Now, we have an upper triangular matrix. Let 
\begin{equation*}
  U=
  \begin{pmatrix}
    1 & 2 & 4\\
    0 & 1 & 1\\
    0 & 0 & 5
  \end{pmatrix}
\end{equation*}
Since we know that we can write Gaussian elimination as a form of matrix multiplication, we have the following equation:
\begin{equation*}
  \begin{pmatrix}
    1 & 0 & 0\\
    0 & 1 & 0\\
    0 & -5 & 1 
  \end{pmatrix}
  \begin{pmatrix}
    1 & 0 & 0\\
    -3 & 1 & 0\\
    -2 & 0 & 1
  \end{pmatrix}
  \begin{pmatrix}
    1 & 2 & 4\\
    3 & 7 & 13\\
    2 & 9 & 18
  \end{pmatrix}
  =
  \begin{pmatrix}
    1 & 2 & 4\\
    0 & 1 & 1\\
    0 & 0 & 5
  \end{pmatrix}
\end{equation*}
By letting 
\begin{equation*}
  l_1=
  \begin{pmatrix}
    1 & 0 & 0\\
    -3 & 1 & 0\\
    -2 & 0 & 1
  \end{pmatrix}
\end{equation*}
and
\begin{equation*}
  l_2=
  \begin{pmatrix}
    1 & 0 & 0\\
    0 & 1 & 0\\
    0 & -5 & 1 
  \end{pmatrix}
  ,
\end{equation*}
we have that 
\begin{equation*}
  l_2l_1A=U
\end{equation*}
Before the next step, we need to know two facts:
\begin{enumerate}
\item If the inverse of a lower triangular matrix exists, i.e. no element on the diagonal is zero, then the inverse of the lower triangular matrix is still a lower triangular matrix.
\item The product of two lower triangular matrix is a lower triangular matrix.
\end{enumerate}
By multiplying the inverses of $l_1$ and $l_2$, we have
\begin{align*}
  l_1^{-1}l_2^{-1}l_2l_1A=l_1^{-1}l_2^{-1}U\\
  A=l_1^{-1}l_2^{-1}U
\end{align*}
Let $L=l_1^{-1}l_2^{-1}$ and we have $A=LU$. Next, find the inverses of $l_1$, $l_2$ and we get:
\begin{align*}
  L&=
     \begin{pmatrix}
       1 & 0 &0 \\
       3 & 1 &0 \\
       2 & 0 &1 
     \end{pmatrix}
               \begin{pmatrix}
                 1 & 0 & 0\\
                 0 & 1 & 0\\
                 0 & 5 & 1 
               \end{pmatrix}\\
   &=\begin{pmatrix}
     1 & 0 &0 \\
     3 & 1 &0 \\
     2 & 5 &1 
   \end{pmatrix}
\end{align*}
Therefore, we have found a LU decomposition for the matrix A, where 
\begin{equation*}
  A =\begin{pmatrix}
    1 & 2 & 4\\
    3 & 7 & 13\\
    2 & 9 & 18
  \end{pmatrix}
  ,
  L=\begin{pmatrix}
    1 & 0 &0 \\
    3 & 1 &0 \\
    2 & 5 &1 
  \end{pmatrix}
  ,
  U=\begin{pmatrix}
    1 & 2 & 4\\
    0 & 1 & 1\\
    0 & 0 & 5
  \end{pmatrix}
\end{equation*}
\subsection{Alternative Method}
\label{subsec:LUD:ALM}
We can also find the LU decomposition of a matrix by using an alternative method.
In this example, we will use the same matrix,$A$, from \ref{subsec:LUD:AGE}.
Since we know that LU decomposition decomposes a matrix into two triangular matrices, we can write:
$$A=LU,$$
where
\begin{equation*}
  L=\begin{pmatrix}
    1 & 0 & 0 \\
    l_{21} & 1 & 0 \\
    l_{31} & l_{32} & 1
  \end{pmatrix}
  ,
  \quad
  U=\begin{pmatrix}
    u_{11} & u_{12} & u_{13}\\
    0 & u_{22} & u_{23}\\
    0 & 0 & u_{33}
  \end{pmatrix}
\end{equation*}
Multiplying $L$ and $U$, we get
\begin{equation*}
  LU=
  \begin{pmatrix}
    u_{11} & u_{12} & u_{13}\\
    l_{21}u_{11} & l_{21}u_{12}+u_{22} & l_{21}u_{13}+u_{23}\\
    l_{31} & l_{31}u_{12}+l_{32}u_{22} & l_{31}u_{13}+l_{32}u_{23}+u_{33}
  \end{pmatrix}
  =
  \begin{pmatrix}
    1 & 2 & 4\\
    3 & 7 & 13\\
    2 & 9 & 18
  \end{pmatrix}
  =A
\end{equation*}
By using simple algebra, we will be able to find $L$ and $U$ respectively.
Going along the first row, we can see that
$u_{11}=1$, $u_{12}=2$, and $u_{13}=4$.
Now considering the second row, we have the following equations:
$$
l_{21}u_{11}=3,
\quad
l_{21}u_{12}+u_{22}=7,
\quad
l_{21}u_{13}+u_{23}=13
$$
The equations above can be solved easily and we get:
$$
l_{21}=3,
\quad
u_{22}=1,
\quad
u_{23}=1
$$
Looking at the third row and substituting known values into the equations, we get:
$$
l_{31}=2,
\quad
2(l_{31})+1(l_{32})=9,
\quad
4(l_{31})+1(l_{32})+u_{33}=18
$$
Solving the above equations, we get:
$$
l_{31}=2,
\quad
l_{32}=5,
\quad
u_{33}=5
$$
Substituting the values back into $L$ and $U$, we have:
\begin{equation*}
  L=\begin{pmatrix}
    1 & 0 & 0 \\
    3 & 1 & 0 \\
    2 & 5 & 1
  \end{pmatrix}
  ,
  \quad
  U=\begin{pmatrix}
    1 & 2 & 4\\
    0 & 1 & 1\\
    0 & 0 & 5
  \end{pmatrix},
\end{equation*}
which is the same as the $L$ and $U$ we computed in \ref{subsec:LUD:AGE}.
\subsection{Pivoting}
\label{subsec:PIV}
In the case where we cannot obtain an upper triangular matrix without swapping rows, we will need to perform LU decomposition with pivoting.
Therefore we will need a permutation matrix. 
A permutation matrix, $P$, is defined as a $nxn$ matrix with precisely one entry whose value is `1' in each row and column, and all the other entries are `0'.
Then we have that:
$$PA=LU,$$
where $P$ is a permutation matrix, $A$ is a $n$ by $n$ matrix, $L$ is an upper triangular matrix and $L$ is a unit lower triangular matrix.
\subsection{Application}
\label{subsec:APP}
Having known the method to perform a LU decomposition, we will now look at the applications of the LU decomposition.
There are a lot of application of LU decomposition in the real world, however, in this section, we will talk about the application in these areas:
\begin{enumerate}
\item Solving system of linear equations
\item Computing the determinant
\item Finding the inverse
\end{enumerate}
\subsubsection{Solving system of linear equations}
\label{subsubsec:LUD:SOL}
Lets say we are given a system of linear equations:
\begin{align*}
  a_{11}x_1+a_{12}x_2+a_{13}x_3=b_1\\
  a_{21}x_1+a_{22}x_2+a_{33}x_3=b_2\\
  a_{31}x_1+a_{32}x_2+a_{33}x_3=b_3,
\end{align*}
we can rewrite it in the form of $Ax=b$, where
\begin{equation*}
  A=
  \begin{pmatrix}
    a_{11} & a_{12} & a_{13}\\
    a_{21} & a_{22} & a_{23}\\
    a_{31} & a_{32} & a_{33}
  \end{pmatrix},
  x=
  \begin{pmatrix}
    x_1\\x_2\\x_3
  \end{pmatrix},
  b=
  \begin{pmatrix}
    b_1\\b_2\\b_3
  \end{pmatrix}
\end{equation*}
Therefore, after performing LU decomposition, we get $LUx=b$.
First, let $Ux=y$, then use the fact that $Ly=b$ to solve for $y$.
Then, solve for $x$ by using $Ux=y$.
The advantage of using this method is that we only need to solve triangular systems, which is very much easier.
\subsubsection{Computing the determinant}
\label{subsubsec:LUD:COM}
Since we know that the determinant of the product of matrices is the product of the determinant of each matrices, in other words:
$$
det(A_1A_2\dots A_n)=det(A_1)det(A_2)\dots det(A_n)
$$
Therefore, computing the determinant of a matrix after undergoing LU decomposition will be very simple since the determinant of a triangular matrix is the product of the elements on the diagonal.
In this case, LU decomposition helps shorten the work needed to calculate the determinant of a matrix.
\subsubsection{Finding the inverse}
\label{subsubsec:LUD:FTI}
Let $A$ be a $n$ by $n$ matrix.
We know that $B$ is the inverse of $A$ if $AB=I$, where $I$ is the identity matrix.
First, by considering the first column of $B$ and $I$, we have:
\begin{equation*}
  A
  \begin{pmatrix}
    b_{11}\\b_{21}\\ b_{31}\\ \vdots \\b_{n1}
  \end{pmatrix}
  =
  \begin{pmatrix}
    1\\0\\0\\ \vdots \\ 0
  \end{pmatrix}
\end{equation*}
Similarly for the second column,
\begin{equation*}
  A
  \begin{pmatrix}
    b_{12}\\b_{22}\\b_{32}\\ \vdots \\b_{n2}
  \end{pmatrix}
  =
  \begin{pmatrix}
    0\\1\\0\\ \vdots \\ 0
  \end{pmatrix}
\end{equation*}
Similarly, we can do the same for the rest of the columns of $B$.
We can use LU decomposition to simplify the calculation for the $n$ different sets of equations to obtain $B$, the inverse of $A$.
% Refrence:
% http://mathforcollege.com/nm/mws/gen/04sle/mws_gen_sle_txt_ludecomp.pdf
% https://ask.fxplus.ac.uk/tools/HELM/pages/workbooks_1_50_jan2008/Workbook30/30_3_lu_decmp.pdf
% http://mathworld.wolfram.com/LUDecomposition.html
% http://mathfaculty.fullerton.edu/mathews/n2003/LUFactorMod.html

\section{Singular Value Decompositions}
\label{sec:svd}
Another useful way of factorising a matrix is through Singular Value Decompositions (SVD).
SVDs decompose a matrix $M$ into three matrices that represent distinct transformations, $M = U \Sigma V^*$.
This decomposition has some useful applications that will be discussed.

\subsection{Motivation and Geometric Interpretation}
\label{sec:svd:1}
To understand the motivation for SVDs it we may consider the geometric interpretation of the SVD.
The fundamental idea here is that an n-dimensional sphere in vector space will produce the image of a hyperellipse (an n-dimenstional generalisation of the ellipse) when it undergoes a linear transform (we will not prove this here).

Figure \ref{fig:svd:1} illustrates this in $\mathbb{R}^2$ over $\mathbb{R}$.
$S$ represents a unit circle, with centre at the origin that undergoes transformation by $A \in M_{2, 2}(\mathbb{R})$.
We have labelled $\mathbf{v_1}, \mathbf{v_2} \in S$, where $\norm{\mathbf{v_1}} = \norm{\mathbf{v_2}} = 1$.
Over on the image side of the figure, we have the unit vectors $\mathbf{u_1}$ and $\mathbf{u_2}$ corresponding to the vectors $\mathbf{v_1}$ and $\mathbf{v_2}$ in the pre-image.
Finally we see that the vectors $\mathbf{u_1}$ and $\mathbf{u_2}$ are scaled by scalars $\sigma_1, \sigma_2 \in \mathbb{F}$ to form the ellipse.


The idea behind SVDs is to decompose the matrix, which  represents linear transform, into three matrices each representing a particular transform on the unit sphere (in the case of figure \ref{fig:svd:1}).

\begin{itemize}
\item Firstly a rotation of the circle by $V^*$.
\item Secondly an elongation or scaling of the circle into an ellipse by $\Sigma$.
\item Lastly a final rotation by $U$.
\end{itemize}

Thus we write $A = U \Sigma V^*$.

\begin{figure}[htb]
nnn  \begin{centering}
    \begin{tikzpicture}
%      \draw [help lines] (0,0) grid (15,6);
      
      \node at (1,1) {\large{S}};
      \coordinate (left) at (3,3);
      \draw [-] (0,3)--(6,3) node[below]{$x$};
      \draw [-] (3,0)--(3,6) node[right]{$y$};
      \draw (left) circle [radius=3/2];
      \draw [->] (left)--($ (left) + (30:3/2) $) node[right] {$\mathbf{v_1}$};
      \draw [->] (left)--($ (left) + (160:3/2) $) node[left] {$\mathbf{v_2}$};
      
      \draw [->, ultra thick] (7,3)--(8,3);
      \node [above] at (7.5,3) {$A$};

      \node at (10,1) {\large{AS}};
      \coordinate (right) at (12,3);
      \draw [-] (9,3)--(15,3) node[below]{$x$};
      \draw [-] (12,0)--(12,6) node[right]{$y$};
      \draw [rotate=20] (right) ellipse [x radius=2, y radius=1];
      \draw [->] (right)--($ (right) + (1.45658529104,1.06224257585) $) node[right] {$\mathbf{\sigma_1 u_1}$};
      \draw [->] (right)--($ (right) + (0.1,1.08) $) node[left] {$\mathbf{\sigma_2 u_2}$};
    \end{tikzpicture}
  \end{centering}
  \caption{A circle in $\mathbb{R}^2$ through linear map $A:\mathbb{R}^2 \to \mathbb{R}^2$ has an ellipse as its image.}
  \label{fig:svd:1}
\end{figure}

It is important to note that the intuition we have described can occur between vector spaces of different dimensions.
For example we may choose a $9 \times 2$ matrix that maps a unit sphere in vector space of dimension $9$ to a vector space of dimension $2$.
We know, however, by the rank nullity theorem that the image of such a transform can have a maximum dimension of $2$ in a nine-dimensional vector space.

\subsection{Abstract Definition}
\cite{trefethen97}
\label{sec:svd:2}

Now we will treat the SVD more formally and define it for finite dimensional real vector space.
For $A \in M_{m,n}(\mathbb{R})$, where $m \geq n$, let  $S \subseteq \mathbb{R}^n$ be the set of points on a unit sphere, we have that $AS$ denotes the set of points on the hyperellipse in $\mathbb{R}^m$.
\begin{itemize}
\item We define the \emph{n singular values} of A to be $\sigma_1, \sigma_2, \dots, \sigma_n$, the principal semiaxes of $AS$ in descending order ($\sigma_1 \leq \sigma_2 \leq \dots \leq \sigma_n$).
\item We define the \emph{n left singular vectors} of $A$: $\mathbf{u_1}, \mathbf{u_2}, \dots, \mathbf{u_n} \in AS$ to be the unit vectors on the principal semiaxes of $AS$ corresponding to $\sigma_1, \sigma_2, \dots, \sigma_n$.
\item We define the \emph{n right singular vectors} of $A$: $\mathbf{v_1}, \mathbf{v_2}, \dots, \mathbf{v_n} \in S$ to be the pre-images of the corresponding vectors $\mathbf{u_1}, \mathbf{u_2}, \dots, \mathbf{u_n}$.
\end{itemize}


As we have seen in the previous section, we have that for $\mathbf{v_j}$, $\mathbf{u_j}$, $\sigma_j$ with $j \leq n$:
\begin{equation}
  \label{eq:svd:1}
  A\mathbf{v_j} = \sigma_j\mathbf{u_j}
\end{equation}
\subsubsection{Reduced SVD}
\label{sec:svd:2:1}

Using this equation (\ref{eq:svd:1}), we can define matrices $V$ and $U$ with columns $\{\mathbf{v_1}, \mathbf{v_2}, \dots, \mathbf{v_n}\}$ and $\{\mathbf{u_1}, \mathbf{u_2}, \dots, \mathbf{u_n}\}$. Then, we have:
\begin{equation}
  \label{eq:svd:2}
  AV  = \hat{U} \hat{\Sigma}
\end{equation}
Where $\hat{\Sigma}$ is the diagonal matrix with entries $\sigma_1, \sigma_2, \dots, \sigma_n$, i.e.\
\[
  \hat{\Sigma} =
  \begin{pmatrix}
    \sigma_1 & 0 & \dots & 0 \\
    0 & \sigma_2 &  \dots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \dots & \sigma_n \\
  \end{pmatrix}
\]

From (\ref{eq:svd:2}) we have the reduced form of the decomposition below from right multiplication by the inverse of $V$.
\[
  A  = \hat{U} \hat{\Sigma} V^*
\]
Where $V^*$ is the inverse of $V$. This is called the reduced SVD because $\hat{U}$ is a not a square matrix for $m > n$ (see section \ref{sec:svd:2:2}).

\subsubsection{Full SVD}
\label{sec:svd:2:2}

While the reduced SVDs serves to factorise a matrix, with all the properties of SVDs intact, SVDs are usually defined such that $U$ is a square matrix and $\Sigma$ is rectangular. It is trivial to extend reduced SVDs to full SVDs.

If $m = n$ we have that $U$ is a unitary matrix. As its columns are unit principal axes for a hyperellipse in $\mathbb{R}^n$, they are orthonormal and span $\mathbb{R}^n$. In this case we have a complete SVD.

In the case where $m > n$, we may arbitrarily append $m - n$ columns that are orthonormal to each other and each of the vectors $\mathbf{u_j}$ for $1 \leq j \leq n$ to the right of the matrix $\hat{U}$. Thus we have orthonormal unitary matrix $U$, whose columns span $\mathbb{R}^n$.

Finally we append $m - n$ columns whose entries are all $0$ below $\hat{\Sigma}$ to acquire $\Sigma$. This will cancel out the arbitrary columns we appended to $\hat{U}$, preserving our equality:
\[
  A = U \Sigma V^*
\]

This is the full Singular Value Decomposition.

\subsubsection{General Definition}
\label{sec:svd:2:3}

So far we have defined SVDs for matrices in $M_{m,n}\mathbb{R}$ where $m \geq n$, we can drop this restriction and define it for the factorisation of $A \in M_{m,n}\mathbb{R}$,
\begin{equation} \label{eq:svd:3}
  A = U \Sigma V^*
\end{equation}
where,
\begin{align*}
  U &\in M_{m,m}(\mathbb{R}) &\text{is unitary}\\
  V &\in M_{n,n}(\mathbb{R}) &\text{is unitary}\\
  \Sigma &\in M_{m,n}(\mathbb{R}) &\text{is diagonal.}\\
\end{align*}

We also have that $\sigma_1, \sigma_2, \dots, \sigma_n$ that form the diagonal of $\Sigma$ are in descending order.

With our singular value decomposition well defined, we can now revisit our geometric interpretation of section \ref{sec:svd:1}. The three transforms on the unit sphere to produce a hyperellipse are seen clearly here. Firstly we have reflection or rotation by unitary $V^*$, second we stretch our circle into a hyperellipse with our diagonal matrix $\Sigma$. Lastly, without changing the shape of the matrix, we again rotate/reflect it by unitary matrix $U$.

This goes to show that any real matrix that can be decomposed as described transforms a sphere to a hyperellipse. It is known that such a decomposition \emph{exists} and is \emph{unique}. \cite{trefethen97}

\subsection{Applications}
\label{sec:svd:3}

The SVD is a very useful decomposition.
We are able to solve many matrix problems by decomposing the matrix like this, as well as apply SVDs to many computing applications such as image compression and page-ranking in web searches.

Fundamentally, a matrix decompositions (including eigendecompositions or LU decompositions) breaks down a complex array of data into smaller, more understandable chunks.
While eigendecomposition and singular value decomposition are similar in that they map the domain to a different basis where they are scaled, they have some key differences; in particular, SVDs can decompose any matrix and the resultant diagonal matrix $\Sigma$ has only real, positive entries.

We will take a look at some particular applications of SVDs.
However, they have many other notable uses such as:
\begin{itemize}
\item  We have that pseudo-inverse $A^+$ (Moore-Penrose) for matrix $A$ satisfies the following criteria (Penrose conditions) \cite{golub2012matrix}.
  \begin{enumerate}
  \item $AA^+A = A$
  \item $A^+AA^+ = A^+$
  \item $(AA^+)^* = AA^+$
  \item $(A^+A)^* = A^+A$
  \end{enumerate}
  The pseudo-inverse is very useful in computing linear least squares, a method that is used in data analysis to find lines of best fit.
  If $A = U \Sigma V^*$, the pseudoinverso of $A$ is given by
  \[
    A = V \Sigma U^*
  \]
\item The singular value decomposition illustrates the image and kernel of the matrix very well.
  For a matrix $A$ that is decomposed into $U \Sigma V^*$ clearly has the basis of its image in the linearly independent columns of $U$ and the kernel's basis in the linearly independent columns of $V$.
\end{itemize}

\subsubsection{Data Compression}
\label{sec:3:ic}

In many computing applications data is stored in arrays and matrices.
To privide an illustrative example, we will use the example of an two-dimensional digital image which is essentially a grid of pixels, each pixel of a certain color.
This image can be represented as a matrix.

For an image $352 \times 240$ pixels large, we can have a corresponding matrix that has 240 rows and 352 columns with each entry representing the color of the corresponding pixel.
We might have that in this matrix there are repeating rows or columns, i.e.\ that the matrix is not linearly independent.
This can be compressed by applying an SVD on the matrix. We note that the matrix will have exactly \emph{n singular values}, where $n$ is the rank of the matrix.

This forms the central idea in the compression of images through SVDs. In reality images and other data are compressed by several algorithms, including Fast Fourier transforms \cite{strang09}.


\section{Sensitivity of Solutions to Linear Equations}
\label{sec:ssl}
\subsection{How sensitive the solution of linear equations is to small changes in the coefficients }
\subsubsection{Computing the perturbed system}
Let

$$Ax=b$$
be our original linear system, where$A\in\mathbb{R}^{n\times n}$ and $b\in\mathbb{R}^n$.
\newline
We then want to change the coefficients of our original system by rewriting the equation to
$$(A+\Delta A)x'=b$$
where $\Delta A\in \mathbb{R}^{n\times n}$ represents the small changes we have made in the coefficient of matrix A.
\newline
Let $\Delta x$ represent the changes to the solutions of the original linear system caused by changing the coefficients, i.e. 
$$\Delta x=x'-x.$$
\newline 
By rearranging we get
$$x'=x+\Delta x.$$
Substituting this back into our perturbed system we get
\begin{align*}
(A+\Delta A)(x+\Delta x)= b\\
Ax+A\Delta x+\Delta A x+\Delta A\Delta x=b
\end{align*}

By substituting $Ax=b$ and rearranging  we get

$$-\Delta A(x+\Delta x)=A\Delta x$$
$$\Rightarrow \Delta x=-A^{-1}\Delta A(x+\Delta x)$$
By taking norms from both side and using the triangle inequality we get
\begin{align*}
\norm{\Delta x}&=\norm{A^{-1}\Delta A(x+\Delta x)}
\\ &\leq \norm{A^{-1}}\norm{\Delta A} \norm{x+ \Delta x}\\
\text{which then gives us}\\
\norm{\Delta x}(1-\norm{A^{-1}}\norm{\Delta A})&\leq\norm{A^{-1}}\norm{\Delta A}\norm{x}
\\\text{Therfore}\\
\frac{\norm{\Delta x}}{\norm{x}} &\leq\frac{\norm{A^{-1}}\norm{\Delta A}}{1-\norm{A^{-1}}\norm{\Delta A}}
=\frac{\norm{A}\norm{A^{-1}}}{(1-\norm{A^{-1}}\norm{\Delta A})}\frac{\norm{\Delta A}}{\norm{A}}
\end{align*}
By \cite{golub2012matrix} we know that the condition number $\kappa (A)$ is defined by
$$\kappa (A)=\norm{A}\norm{A^{-1}}$$
Our original inequality then turns into


%https://researcher.watson.ibm.com/researcher/files/ie-jakub.marecek/ch04_LinearSystems_handout.pdf
\section{Conclusion}
\bibliographystyle{ieeetr}
\bibliography{refjh.bib,refjk.bib}
\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End: