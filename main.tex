\documentclass[12pt,reqno,twoside,titlepage]{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[square,numbers,sort&compress]{natbib}
\usepackage{amsmath}
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{% 
\hskip -\arraycolsep
\let\@ifnextchar\new@ifnextchar
\array{#1}}
\makeatother
\makeatletter
\renewcommand*\env@matrix[1][\arraystretch]{%
\edef\arraystretch{#1}


\usepackage{amsmath,amsfonts,graphicx,amssymb,amsthm,tikz,csquotes}
\theoremstyle{definition}
\newtheorem{exmp}{Example}[section]

\usetikzlibrary{calc,arrows}

\begin{document}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\title{Matrix Decompositions}
\author{George Keeble
  \and
  Jishnu Kaiwar
  \and
  Bing En Gan
  \and
  Jevvy Huang
}
\date{January 2019}

\maketitle

\begin{abstract}
  abstract here
\end{abstract}

\section{Introduction}
\label{sec:int}
the quick brown fox jumped over the lazy dog.

\section{Gaussian Elimination}
\label{sec:gaus}

Gaussian elimination (or row reduction) is used for solving systems of equations in linear algebra, it also plays a part in evaluating the rank, determinant and consequently the inverse of a matrix.
This algorithm is attributed to the mathematician Carl Gauss (1777-1855) however there is some speculation that some Chinese mathematicians were using this string of thought as early as 179 A.D. which I shall touch on breifly when discussing ‘Fangcheng’.
In order to compute the algorithm one must have a grounded understanding of elementary matrices and possible elementary row operations.
There are three elementary row operations;

\begin{enumerate}
\item Swapping two rows
\item Multiplying a row by $\lambda \in \mathbb{R}$ such that $\lambda \ne 0$
\item Adding a multiple of one row to another row
\end{enumerate}

Each of these elementary row operations (ERO’s) can be expressed as an elementary matrix often denoted ‘E’. When performing an ERO we are essentially multiplying our matrix A by our elementary matrix E. Below we see a basic example.

\begin{exmp}
\begin{equation*}
 A = 
\begin{pmatrix}
 1 & 0 & 2 & 3 \\
 2 & -1 & 3 & 6 \\
 1 & 4 & 4 & 0
\end{pmatrix}
   ,
  \quad
  E = 
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
3 & 0 & 1
\end{pmatrix}
\end{equation*}
\newline
\begin{equation*}
\implies EA = 
\begin{pmatrix}
1 & 0 & 2 & 3 \\
2 & -1 & 3 & 6 \\
4 & 4 & 10 & 0
\end{pmatrix}
\end{equation*}
\newline
By simple matrix multiplication, here multiplying our matrix A by the elementary matrix E yields the same result as computing the ERO, “adding three times the 1st row to the 3rd row”.
\end{exmp}

Computing a finite number of ERO’s or multiplying a matrix A by a finite number of elementary matrices we will eventually arrive in echelon form. In order for a matrix to be in echelon form it must satisfy these conditions;

\begin{enumerate}
    \item in each row there is a leading 1
    \item if row i is above row j then the leading 1 in row i is to the left of row j’s leading 1
    \item the matrix is in reduced row echelon form if it satisfies 1. and 2. and also in each column with a leading 1 all other elements are 0.
\end{enumerate}

We now need to define an upper triangular matrix, it is a matrix where all elements below the main diagonal are 0, if a matrix is upper triangular then it is in row echelon form. This is an easy way to know purely from observation whether a matrix is in echelon form.

Gaussian elimination has many applications, its most simple is arguably its ability to simplify a system of linear equations in order to determine a set of solutions, depending on the structure of the matrix in echelon form alters the solutions of a system of equations. We shall know go through the process and discuss the solutions and why they have the nature they possess. Consider the equation $A\textbf{x} = \textbf{b}$

\begin{equation*}
    \begin{pmatrix}
    a_{11} & a_{12} & a_{13} & \dots & a_{1m} \\
    a_{21} & a_{22} & a_{23} & \dots & a_{2m} \\
    a_{31} & a_{32} & a_{33} & \dots & a_{3m} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    a_{n1} & a_{n2} & a_{n3} & \dots & a_{nm}
    \end{pmatrix}
    \begin{pmatrix}
    x_{1} \\
    x_{2} \\
    x_{3} \\
    \vdots \\
    x_{n}
    \end{pmatrix}
    =
    \begin{pmatrix}
    b_{1} \\
    b_{2} \\
    b_{3} \\
    \vdots \\
    b_{n}
    \end{pmatrix}
\end{equation*}
\newline
First we form an augmented matrix so that we can begin to compute ERO’s.
\begin{equation*}
\begin{pmatrix}[ccccc|c]
a_{11} & a_{12} & a_{13} & \dots & a_{1m} & b_{1} \\
a_{21} & a_{22} & a_{23} & \dots & a_{2m} & b_{2} \\
a_{31} & a_{32} & a_{33} & \dots & a_{3m} & b_{3} \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
a_{n1} & a_{n2} & a_{n3} & \dots & a_{nm} & b_{n}
\end{pmatrix}
=
\begin{pmatrix}
x_{1} \\
x_{2} \\
x_{3} \\
\vdots \\
x_{n}
\end{pmatrix}
\end{equation*}
We then compute a finite series of ERO’s until our augmented matrix is in upper triangular (or echelon) form as seen below.
\begin{equation*}
    \begin{pmatrix}[ccccc|c]
    a’_{11} & a’_{12} & a’_{13} & \dots & a’_{1m} & b’_{1} \\
    0 & a’_{22} & a’_{23} & \dots & a’_{2m} & b’_{2} \\
    0 & 0 & a’_{33} & \dots & a’_{3m} & b’_{3} \\
    \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & 0 & \dots & a’_{nm} & b’_{n}
    \end{pmatrix}
\end{equation*}

\newpage
\section{LU Decomposition}
\label{sec:LUD}
\subsection{Basic Concept}
\label{subsec:LUD:BAC}
The LU decomposition is the process of factorizing a given square matrix, $A$ into a unit lower triangular matrix, $L$ and an upper triangular matrix, $U$.
From \ref{sec:gaus}, the main point of Gaussian elimination is to convert a matrix into a triangular matrix.
The LU decomposition is an extension of the Gaussian elimination algorithm in order to take advantage of the triangular matrix system.
\newline
Suppose that we have a square matrix $A$, we can write:
$$A = LU,$$ 
where
\begin{equation*}
  L=\begin{pmatrix}
    1 & 0 & 0 \\
    l_{21} & 1 & 0 \\
    l_{31} & l_{32} & 1
  \end{pmatrix}
  ,
  \quad
  U=\begin{pmatrix}
    u_{11} & u_{12} & u_{13}\\
    0 & u_{22} & u_{23}\\
    0 & 0 & u_{33}
  \end{pmatrix}
\end{equation*}
\begin{center}
  \begin{equation*}
    l_{21}, l_{31}, l_{32}, u_{11}, u_{12}, u_{13}, u_{22}, u_{23}, u_{33} \in \mathbb{R};
  \end{equation*}
  \begin{equation*}
    u_{11}, u_{22}, u_{33} \neq 0
  \end{equation*}
\end{center}
The motivation of performing a LU decomposition is that triangular matrices are easier to deal with.
For example, solving linear system of equations, finding the inverse of a matrix, computing the determinant of a matrix, will be significantly easier with LU decomposition.
\paragraph{Remark}
We have to note that a matrix with determinant $0$ will not have a LU decomposition.
This is because we cannot have an element with value $0$ in the diagonal of both the triangular matrices.
\subsection{Applying Gaussian Elimination}
\label{subsec:LUD:AGE}
For an example, let
\begin{equation*}
  A=
  \begin{pmatrix}
    1 & 2 & 4\\
    3 & 7 & 13\\
    2 & 9 & 18
  \end{pmatrix}
\end{equation*}
and we want to find the LU decomposition of the matrix $A$.
First, we apply Gaussian elimination without swapping rows.
By subtracting 3 multiplied row 1 from row 2 and 2 multiplied row 1 from row 3,
\begin{equation*}
  \begin{pmatrix}
    1 & 2 & 4\\
    3 & 7 & 13\\
    2 & 9 & 18
  \end{pmatrix}
  \sim
  \begin{pmatrix}
    1 & 2 & 4\\
    0 & 1 & 1\\
    0 & 5 & 10
  \end{pmatrix}
\end{equation*}
Then, we subtract 5 multiplied row 2 from row 3,
\begin{equation*}
  \begin{pmatrix}
    1 & 2 & 4\\
    0 & 1 & 1\\
    0 & 5 & 10
  \end{pmatrix}
  \sim
  \begin{pmatrix}
    1 & 2 & 4\\
    0 & 1 & 1\\
    0 & 0 & 5
  \end{pmatrix}
\end{equation*}
Now, we have an upper triangular matrix. Let 
\begin{equation*}
  U=
  \begin{pmatrix}
    1 & 2 & 4\\
    0 & 1 & 1\\
    0 & 0 & 5
  \end{pmatrix}
\end{equation*}
Since we know that we can write Gaussian elimination as a form of matrix multiplication, we have the following equation:
\begin{equation*}
  \begin{pmatrix}
    1 & 0 & 0\\
    0 & 1 & 0\\
    0 & -5 & 1 
  \end{pmatrix}
  \begin{pmatrix}
    1 & 0 & 0\\
    -3 & 1 & 0\\
    -2 & 0 & 1
  \end{pmatrix}
  \begin{pmatrix}
    1 & 2 & 4\\
    3 & 7 & 13\\
    2 & 9 & 18
  \end{pmatrix}
  =
  \begin{pmatrix}
    1 & 2 & 4\\
    0 & 1 & 1\\
    0 & 0 & 5
  \end{pmatrix}
\end{equation*}
By letting 
\begin{equation*}
  l_1=
  \begin{pmatrix}
    1 & 0 & 0\\
    -3 & 1 & 0\\
    -2 & 0 & 1
  \end{pmatrix}
\end{equation*}
and
\begin{equation*}
  l_2=
  \begin{pmatrix}
    1 & 0 & 0\\
    0 & 1 & 0\\
    0 & -5 & 1 
  \end{pmatrix}
  ,
\end{equation*}
we have that 
\begin{equation*}
  l_2l_1A=U
\end{equation*}
Before the next step, we need to know two facts:
\begin{enumerate}
\item If the inverse of a lower triangular matrix exists, i.e. no element on the diagonal is zero, then the inverse of the lower triangular matrix is still a lower triangular matrix.
\item The product of two lower triangular matrix is a lower triangular matrix.
\end{enumerate}
By multiplying the inverses of $l_1$ and $l_2$, we have
\begin{align*}
  l_1^{-1}l_2^{-1}l_2l_1A=l_1^{-1}l_2^{-1}U\\
  A=l_1^{-1}l_2^{-1}U
\end{align*}
Let $L=l_1^{-1}l_2^{-1}$ and we have $A=LU$. Next, find the inverses of $l_1$, $l_2$ and we get:
\begin{align*}
  L&=
     \begin{pmatrix}
       1 & 0 &0 \\
       3 & 1 &0 \\
       2 & 0 &1 
     \end{pmatrix}
               \begin{pmatrix}
                 1 & 0 & 0\\
                 0 & 1 & 0\\
                 0 & 5 & 1 
               \end{pmatrix}\\
   &=\begin{pmatrix}
     1 & 0 &0 \\
     3 & 1 &0 \\
     2 & 5 &1 
   \end{pmatrix}
\end{align*}
Therefore, we have found a LU decomposition for the matrix A, where 
\begin{equation*}
  A =\begin{pmatrix}
    1 & 2 & 4\\
    3 & 7 & 13\\
    2 & 9 & 18
  \end{pmatrix}
  ,
  L=\begin{pmatrix}
    1 & 0 &0 \\
    3 & 1 &0 \\
    2 & 5 &1 
  \end{pmatrix}
  ,
  U=\begin{pmatrix}
    1 & 2 & 4\\
    0 & 1 & 1\\
    0 & 0 & 5
  \end{pmatrix}
\end{equation*}
\subsection{Alternative Method}
\label{subsec:LUD:ALM}
We can also find the LU decomposition of a matrix by using an alternative method.
In this example, we will use the same matrix,$A$, from \ref{subsec:LUD:AGE}.
Since we know that LU decomposition decomposes a matrix into two triangular matrices, we can write:
$$A=LU,$$
where
\begin{equation*}
  L=\begin{pmatrix}
    1 & 0 & 0 \\
    l_{21} & 1 & 0 \\
    l_{31} & l_{32} & 1
  \end{pmatrix}
  ,
  \quad
  U=\begin{pmatrix}
    u_{11} & u_{12} & u_{13}\\
    0 & u_{22} & u_{23}\\
    0 & 0 & u_{33}
  \end{pmatrix}
\end{equation*}
Multiplying $L$ and $U$, we get
\begin{equation*}
  LU=
  \begin{pmatrix}
    u_{11} & u_{12} & u_{13}\\
    l_{21}u_{11} & l_{21}u_{12}+u_{22} & l_{21}u_{13}+u_{23}\\
    l_{31} & l_{31}u_{12}+l_{32}u_{22} & l_{31}u_{13}+l_{32}u_{23}+u_{33}
  \end{pmatrix}
  =
  \begin{pmatrix}
    1 & 2 & 4\\
    3 & 7 & 13\\
    2 & 9 & 18
  \end{pmatrix}
  =A
\end{equation*}
By using simple algebra, we will be able to find $L$ and $U$ respectively.
Going along the first row, we can see that
$u_{11}=1$, $u_{12}=2$, and $u_{13}=4$.
Now considering the second row, we have the following equations:
$$
l_{21}u_{11}=3,
\quad
l_{21}u_{12}+u_{22}=7,
\quad
l_{21}u_{13}+u_{23}=13
$$
The equations above can be solved easily and we get:
$$
l_{21}=3,
\quad
u_{22}=1,
\quad
u_{23}=1
$$
Looking at the third row and substituting known values into the equations, we get:
$$
l_{31}=2,
\quad
2(l_{31})+1(l_{32})=9,
\quad
4(l_{31})+1(l_{32})+u_{33}=18
$$
Solving the above equations, we get:
$$
l_{31}=2,
\quad
l_{32}=5,
\quad
u_{33}=5
$$
Substituting the values back into $L$ and $U$, we have:
\begin{equation*}
  L=\begin{pmatrix}
    1 & 0 & 0 \\
    3 & 1 & 0 \\
    2 & 5 & 1
  \end{pmatrix}
  ,
  \quad
  U=\begin{pmatrix}
    1 & 2 & 4\\
    0 & 1 & 1\\
    0 & 0 & 5
  \end{pmatrix},
\end{equation*}
which is the same as the $L$ and $U$ we computed in \ref{subsec:LUD:AGE}.
\subsection{Pivoting}
\label{subsec:PIV}
In the case where we cannot obtain an upper triangular matrix without swapping rows, we will need to perform LU decomposition with pivoting.
Therefore we will need a permutation matrix. 
A permutation matrix, $P$, is defined as a $nxn$ matrix with precisely one entry whose value is `1' in each row and column, and all the other entries are `0'.
Then we have that:
$$PA=LU,$$
where $P$ is a permutation matrix, $A$ is a $n$ by $n$ matrix, $L$ is an upper triangular matrix and $L$ is a unit lower triangular matrix.
\subsection{Application}
\label{subsec:APP}
Having known the method to perform a LU decomposition, we will now look at the applications of the LU decomposition.
There are a lot of application of LU decomposition in the real world, however, in this section, we will talk about the application in these areas:
\begin{enumerate}
\item Solving system of linear equations
\item Computing the determinant
\item Finding the inverse
\end{enumerate}
\subsubsection{Solving system of linear equations}
\label{subsubsec:LUD:SOL}
Lets say we are given a system of linear equations:
\begin{align*}
  a_{11}x_1+a_{12}x_2+a_{13}x_3=b_1\\
  a_{21}x_1+a_{22}x_2+a_{33}x_3=b_2\\
  a_{31}x_1+a_{32}x_2+a_{33}x_3=b_3,
\end{align*}
we can rewrite it in the form of $Ax=b$, where
\begin{equation*}
  A=
  \begin{pmatrix}
    a_{11} & a_{12} & a_{13}\\
    a_{21} & a_{22} & a_{23}\\
    a_{31} & a_{32} & a_{33}
  \end{pmatrix},
  x=
  \begin{pmatrix}
    x_1\\x_2\\x_3
  \end{pmatrix},
  b=
  \begin{pmatrix}
    b_1\\b_2\\b_3
  \end{pmatrix}
\end{equation*}
Therefore, after performing LU decomposition, we get $LUx=b$.
First, let $Ux=y$, then use the fact that $Ly=b$ to solve for $y$.
Then, solve for $x$ by using $Ux=y$.
The advantage of using this method is that we only need to solve triangular systems, which is very much easier.
\subsubsection{Computing the determinant}
\label{subsubsec:LUD:COM}
Since we know that the determinant of the product of matrices is the product of the determinant of each matrices, in other words:
$$
det(A_1A_2\dots A_n)=det(A_1)det(A_2)\dots det(A_n)
$$
Therefore, computing the determinant of a matrix after undergoing LU decomposition will be very simple since the determinant of a triangular matrix is the product of the elements on the diagonal.
In this case, LU decomposition helps shorten the work needed to calculate the determinant of a matrix.
\subsubsection{Finding the inverse}
\label{subsubsec:LUD:FTI}
Let $A$ be a $n$ by $n$ matrix.
We know that $B$ is the inverse of $A$ if $AB=I$, where $I$ is the identity matrix.
First, by considering the first column of $B$ and $I$, we have:
\begin{equation*}
  A
  \begin{pmatrix}
    b_{11}\\b_{21}\\ b_{31}\\ \vdots \\b_{n1}
  \end{pmatrix}
  =
  \begin{pmatrix}
    1\\0\\0\\ \vdots \\ 0
  \end{pmatrix}
\end{equation*}
Similarly for the second column,
\begin{equation*}
  A
  \begin{pmatrix}
    b_{12}\\b_{22}\\b_{32}\\ \vdots \\b_{n2}
  \end{pmatrix}
  =
  \begin{pmatrix}
    0\\1\\0\\ \vdots \\ 0
  \end{pmatrix}
\end{equation*}
Similarly, we can do the same for the rest of the columns of $B$.
We can use LU decomposition to simplify the calculation for the $n$ different sets of equations to obtain $B$, the inverse of $A$.
% Refrence:
% http://mathforcollege.com/nm/mws/gen/04sle/mws_gen_sle_txt_ludecomp.pdf
% https://ask.fxplus.ac.uk/tools/HELM/pages/workbooks_1_50_jan2008/Workbook30/30_3_lu_decmp.pdf
% http://mathworld.wolfram.com/LUDecomposition.html
% http://mathfaculty.fullerton.edu/mathews/n2003/LUFactorMod.html

\section{Singular Value Decompositions}
\label{sec:svd}
Another useful way of factorising a matrix is through Singular Value Decompositions (SVD).
SVDs decompose a matrix $M$ into three matrices that represent distinct transformations, $M = U \Sigma V^*$.
This decomposition has some useful applications that will be discussed.

\subsection{Motivation and Geometric Interpretation}
\label{sec:svd:1}
To understand the motivation for SVDs it we may consider the geometric interpretation of the SVD.
The fundamental idea here is that an n-dimensional sphere in vector space will produce the image of a hyperellipse (an n-dimenstional generalisation of the ellipse) when it undergoes a linear transform (we will not prove this here).

Figure \ref{fig:svd:1} illustrates this in $\mathbb{R}^2$ over $\mathbb{R}$.
$S$ represents a unit circle, with centre at the origin that undergoes transformation by $A \in M_{2, 2}(\mathbb{R})$.
We have labelled $\mathbf{v_1}, \mathbf{v_2} \in S$, where $\norm{\mathbf{v_1}} = \norm{\mathbf{v_2}} = 1$.
Over on the image side of the figure, we have the unit vectors $\mathbf{u_1}$ and $\mathbf{u_2}$ corresponding to the vectors $\mathbf{v_1}$ and $\mathbf{v_2}$ in the pre-image.
Finally we see that the vectors $\mathbf{u_1}$ and $\mathbf{u_2}$ are scaled by scalars $\sigma_1, \sigma_2 \in \mathbb{F}$ to form the ellipse.


The idea behind SVDs is to decompose the matrix, which  represents linear transform, into three matrices each representing a particular transform on the unit sphere (in the case of figure \ref{fig:svd:1}).

\begin{itemize}
\item Firstly a rotation of the circle by $V^*$.
\item Secondly an elongation or scaling of the circle into an ellipse by $\Sigma$.
\item Lastly a final rotation by $U$.
\end{itemize}

Thus we write $A = U \Sigma V^*$.

\begin{figure}[htb]
  \begin{centering}
    \begin{tikzpicture}
%      \draw [help lines] (0,0) grid (15,6);
      
      \node at (1,1) {\large{S}};
      \coordinate (left) at (3,3);
      \draw [-] (0,3)--(6,3) node[below]{$x$};
      \draw [-] (3,0)--(3,6) node[right]{$y$};
      \draw (left) circle [radius=3/2];
      \draw [->] (left)--($ (left) + (30:3/2) $) node[right] {$\mathbf{v_1}$};
      \draw [->] (left)--($ (left) + (160:3/2) $) node[left] {$\mathbf{v_2}$};
      
      \draw [->, ultra thick] (7,3)--(8,3);
      \node [above] at (7.5,3) {$A$};

      \node at (10,1) {\large{AS}};
      \coordinate (right) at (12,3);
      \draw [-] (9,3)--(15,3) node[below]{$x$};
      \draw [-] (12,0)--(12,6) node[right]{$y$};
      \draw [rotate=20] (right) ellipse [x radius=2, y radius=1];
      \draw [->] (right)--($ (right) + (1.45658529104,1.06224257585) $) node[right] {$\mathbf{\sigma_1 u_1}$};
      \draw [->] (right)--($ (right) + (0.1,1.08) $) node[left] {$\mathbf{\sigma_2 u_2}$};
    \end{tikzpicture}
  \end{centering}
  \caption{A circle in $\mathbb{R}^2$ through linear map $A:\mathbb{R}^2 \to \mathbb{R}^2$ has an ellipse as its image.}
  \label{fig:svd:1}
\end{figure}

It is important to note that the intuition we have described can occur between vector spaces of different dimensions.
For example we may choose a $9 \times 2$ matrix that maps a unit sphere in vector space of dimension $9$ to a vector space of dimension $2$.
We know, however, by the rank nullity theorem that the image of such a transform can have a maximum dimension of $2$ in a nine-dimensional vector space.

\subsection{Abstract Definition}
\cite{trefethen97}
\label{sec:svd:2}

Now we will treat the SVD more formally and define it for finite dimensional real vector space.
For $A \in M_{m,n}(\mathbb{R})$, where $m \geq n$, let  $S \subseteq \mathbb{R}^n$ be the set of points on a unit sphere, we have that $AS$ denotes the set of points on the hyperellipse in $\mathbb{R}^m$.
\begin{itemize}
\item We define the \emph{n singular values} of A to be $\sigma_1, \sigma_2, \dots, \sigma_n$, the principal semiaxes of $AS$ in descending order ($\sigma_1 \leq \sigma_2 \leq \dots \leq \sigma_n$).
\item We define the \emph{n left singular vectors} of $A$: $\mathbf{u_1}, \mathbf{u_2}, \dots, \mathbf{u_n} \in AS$ to be the unit vectors on the principal semiaxes of $AS$ corresponding to $\sigma_1, \sigma_2, \dots, \sigma_n$.
\item We define the \emph{n right singular vectors} of $A$: $\mathbf{v_1}, \mathbf{v_2}, \dots, \mathbf{v_n} \in S$ to be the pre-images of the corresponding vectors $\mathbf{u_1}, \mathbf{u_2}, \dots, \mathbf{u_n}$.
\end{itemize}


As we have seen in the previous section, we have that for $\mathbf{v_j}$, $\mathbf{u_j}$, $\sigma_j$ with $j \leq n$:
\begin{equation}
  \label{eq:svd:1}
  A\mathbf{v_j} = \sigma_j\mathbf{u_j}
\end{equation}
\subsubsection{Reduced SVD}
\label{sec:svd:2:1}

Using this equation (\ref{eq:svd:1}), we can define matrices $V$ and $U$ with columns $\{\mathbf{v_1}, \mathbf{v_2}, \dots, \mathbf{v_n}\}$ and $\{\mathbf{u_1}, \mathbf{u_2}, \dots, \mathbf{u_n}\}$. Then, we have:
\begin{equation}
  \label{eq:svd:2}
  AV  = \hat{U} \hat{\Sigma}
\end{equation}
Where $\hat{\Sigma}$ is the diagonal matrix with entries $\sigma_1, \sigma_2, \dots, \sigma_n$, i.e.\
\[
  \hat{\Sigma} =
  \begin{pmatrix}
    \sigma_1 & 0 & \dots & 0 \\
    0 & \sigma_2 &  \dots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \dots & \sigma_n \\
  \end{pmatrix}
\]

From (\ref{eq:svd:2}) we have the reduced form of the decomposition below from right multiplication by the inverse of $V$.
\[
  A  = \hat{U} \hat{\Sigma} V^*
\]
Where $V^*$ is the inverse of $V$. This is called the reduced SVD because $\hat{U}$ is a not a square matrix for $m > n$ (see section \ref{sec:svd:2:2}).

\subsubsection{Full SVD}
\label{sec:svd:2:2}

While the reduced SVDs serves to factorise a matrix, with all the properties of SVDs intact, SVDs are usually defined such that $U$ is a square matrix and $\Sigma$ is rectangular. It is trivial to extend reduced SVDs to full SVDs.

If $m = n$ we have that $U$ is a unitary matrix. As its columns are unit principal axes for a hyperellipse in $\mathbb{R}^n$, they are orthonormal and span $\mathbb{R}^n$. In this case we have a complete SVD.

In the case where $m > n$, we may arbitrarily append $m - n$ columns that are orthonormal to each other and each of the vectors $\mathbf{u_j}$ for $1 \leq j \leq n$ to the right of the matrix $\hat{U}$. Thus we have orthonormal unitary matrix $U$, whose columns span $\mathbb{R}^n$.

Finally we append $m - n$ columns whose entries are all $0$ below $\hat{\Sigma}$ to acquire $\Sigma$. This will cancel out the arbitrary columns we appended to $\hat{U}$, preserving our equality:
\[
  A = U \Sigma V^*
\]

This is the full Singular Value Decomposition.

\subsubsection{General Definition}
\label{sec:svd:2:3}

So far we have defined SVDs for matrices in $M_{m,n}\mathbb{R}$ where $m \geq n$, we can drop this restriction and define it for the factorisation of $A \in M_{m,n}\mathbb{R}$,
\[
  A = U \Sigma V^*
\]
where,
\begin{align*}
  U &\in M_{m,m}(\mathbb{R}) &\text{is unitary}\\
  V &\in M_{n,n}(\mathbb{R}) &\text{is unitary}\\
  \Sigma &\in M_{m,n}(\mathbb{R}) &\text{is diagonal.}\\
\end{align*}

We also have that $\sigma_1, \sigma_2, \dots, \sigma_n$ that form the diagonal of $\Sigma$ are in descending order.

With our singular value decomposition well defined, we can now revisit our geometric interpretation of section \ref{sec:svd:1}. The three transforms on the unit sphere to produce a hyperellipse are seen clearly here. Firstly we have reflection or rotation by unitary $V^*$, second we stretch our circle into a hyperellipse with our diagonal matrix $\Sigma$. Lastly, without changing the shape of the matrix, we again rotate/reflect it by unitary matrix $U$.

This goes to show that any real matrix that can be decomposed as described transforms a sphere to a hyperellipse. In our next section we will show that every matrix can be decomposed in such a way.
\subsection{Existence and Uniqueness}
\label{sec:svd:3}

Lorem Ipsum.

\section{Sensitivity of Solutions to Linear Equations}
\label{sec:ssl}
\subsection{How sensitive the solution of linear equations is to small changes in the coefficients }
\subsubsection{Computing the perturbed system}
Let

$$Ax=b$$
be our original linear system, where$A\in\mathbb{R}^{n\times n}$ and $b\in\mathbb{R}^n$.
\newline
We then want to change the coefficients of our original system by rewriting the equation to
$$(A+\Delta A)x'=b$$
where $\Delta A\in \mathbb{R}^{n\times n}$ represents the small changes we have made in the coefficient of matrix A.
\newline
Let $\Delta x$ represent the changes to the solutions of the original linear system caused by changing the coefficients, i.e. 
$$\Delta x=x'-x.$$
\newline 
By rearranging we get
$$x'=x+\Delta x.$$
Substituting this back into our perturbed system we get
\begin{align*}
(A+\Delta A)(x+\Delta x)= b\\
Ax+A\Delta x+\Delta A x+\Delta A\Delta x=b
\end{align*}

By substituting $Ax=b$ and rearranging  we get

$$-\Delta A(x+\Delta x)=A\Delta x$$
$$\Rightarrow \Delta x=-A^{-1}\Delta A(x+\Delta x)$$
By taking norms from both side and using the triangle inequality we get
\begin{align*}
\norm{\Delta x}&=\norm{A^{-1}\Delta A(x+\Delta x)}
\\ &\leq \norm{A^{-1}}\norm{\Delta A} \norm{x+ \Delta x}\\
\text{which then gives us}\\
\norm{\Delta x}(1-\norm{A^{-1}}\norm{\Delta A})&\leq\norm{A^{-1}}\norm{\Delta A}\norm{x}
\\\text{Therfore}\\
\frac{\norm{\Delta x}}{\norm{x}} &\leq\frac{\norm{A^{-1}}\norm{\Delta A}}{1-\norm{A^{-1}}\norm{\Delta A}}
=\frac{\norm{A}\norm{A^{-1}}}{(1-\norm{A^{-1}}\norm{\Delta A})}\frac{\norm{\Delta A}}{\norm{A}}
\end{align*}
By \cite{golub2012matrix} we know that the condition number $\kappa (A)$ is defined by
$$\kappa (A)=\norm{A}\norm{A^{-1}}$$
Our original inequality then turns into
$$\frac{\norm{\Delta x}}{\norm{x}}\leq\frac{\kappa (A)}{1-\kappa(A)}\frac{\norm{\Delta A}}{\norm{A}}$$
%http://terpconnect.umd.edu/~petersd/460/linsysterrn.pdf
%https://www.physicsforums.com/threads/norm-of-operator-vs-norm-of-its-inverse.487496/
%https://www2.math.uconn.edu/~leykekhman/courses/MATH3795/Lectures/Lecture_6_Linear_system_error.pdf
%https://researcher.watson.ibm.com/researcher/files/ie-jakub.marecek/ch04_LinearSystems_handout.pdf

\section{Conclusion}
\bibliographystyle{ieeetr}
\bibliography{refjh.bib,refjk.bib}
\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End: